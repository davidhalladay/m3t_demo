
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>M3T</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script src="js/app.js"></script>
    
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>M<sup>3</sup>T</b>: Multi-Scale Memory Matching for Video Object <br> Segmentation and Tracking </br> 
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://www.cs.ubc.ca/~rgoyal14/" target="_blank" rel="noopener noreferrer">
                            Raghav Goyal<sup><b>*</b></sup> \(^{1, 2}\)
                        </a>
                    </li>
                    <li>
                        <a href="https://sites.google.com/view/wancyuanfan" target="_blank" rel="noopener noreferrer">
                            Wan-Cyuan Fan<sup><b>*</b></sup> \(^{1, 2}\)
                        </a>
                    </li>
                    <li>
                        <a href="https://msiam.github.io/homepage/" target="_blank" rel="noopener noreferrer">
                          Mennatullah Siam \(^{4}\)
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cs.ubc.ca/~lsigal/" target="_blank" rel="noopener noreferrer">
                            Leonid Sigal \(^{1, 2, 3}\)
                        </a>
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        \(^1\) University of British Columbia
                    </li>
                    <li>
                        \(^2\) Vector Institute for AI
                    </li>
                    <li>
                        \(^3\) CIFAR AI Chair
                    </li>
                    <li>
                        \(^4\) Ontario Tech University
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                \(^*\) Equal Contribution
            </div>
        </div>
        <div class="row">
            <h2 class="col-md-12 text-center">
                <small>
                    Under submission
                </small>
            </h2>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2312.08514" target="_blank" rel="noopener noreferrer">
                            <image src="img/m3t_paper_cover.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="" target="_blank" rel="noopener noreferrer">
                            <image src="img/logos_huggingface.png" height="60px">
                                <h4><strong>Web demo (coming soon)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="" target="_blank" rel="noopener noreferrer">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code (coming soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <!-- Video -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/aHbdpb_IcPA?controls=0&showinfo=0&autoplay=1&loop=1&mute=1&playlist=aHbdpb_IcPA" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <!-- TL;DR, -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <h3>
                    TL;DR
                </h3>
                <p class="text-justify">
                    M<sup>3</sup>T is a video object tracking model that, 
                    with the initial segmentation mask and a video, 
                    accurately tracks the specified object throughout, even amidst object transformations.
                </p>
            </div>
        </div>

        <!-- Abstraction, Overview figure-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <image src="img/overview.png" class="img-responsive" alt="overview"><br> -->
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Video Object Segmentation (VOS) has became increasingly important with availability of larger datasets and more complex and realistic settings, 
                    which involve long videos with global motion (e.g, in egocentric settings), depicting small objects undergoing both rigid and non-rigid (including state) deformations. 
                    While a number of recent approaches have been explored for this task, these data characteristics still present challenges. In this work we propose a novel, 
                    DETR-style encoder-decoder architecture, which focuses on systematically analyzing and addressing aforementioned challenges. 
                    Specifically, our model enables on-line inference with long videos in a windowed fashion, by breaking the video into clips and propagating context among them using time-coded memory. 
                    We illustrate that short clip length and longer memory with learned time-coding are important design choices for achieving state-of-the-art (SoTA) performance. 
                    Further, we propose multi-scale matching and decoding to ensure sensitivity and accuracy for small objects. 
                    Finally, we propose a novel training strategy that focuses learning on portions of the video where an object undergoes significant deformations -- a form of "soft" hard-negative mining, 
                    implemented as loss-reweighting. Collectively, these technical contributions allow our model to achieve SoTA performance on two complex datasets -- VISOR and VOST. 
                    A series of detailed ablations validate our design choices as well as provide insights into the importance of parameter choices and their impact on performance.
                </p>
            </div>
        </div>

        <!-- Methodology -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Methodology
                </h3>
                <image src="img/framework.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    We divide an input video into non-overlapping clips of length L. 
                    For a query clip, we retrieve information on previous clips from our (a) <b>Clip-based Memory</b> in the form of frames and predicted (or initial reference) masks. 
                    We use a 2D-CNN backbone to obtain features for query frames X<sup>q</sup>, and features for memory frames and masks, X<sup>M</sup> and Y<sup>M</sup> respectively. 
                    We then use our proposed (b) <b>Multi-Scale Matching Encoder</b> to perform dense matching at multiple-scales with frame features of the query clip X<sup>q</sup> and memory frames X<sup>M</sup>, 
                    and use the resulting similarity between frames to obtain query clip’s mask features Y<sup>q,enc</sup> as a weighted combination of the memory mask features Y<sup>M</sup>. 
                    In doing so, we modulate the similarity using our proposed <b>Relative-Time Encoding</b> (RTE) to learn recency of information in memory, thereby facilitating propagation over long-time spans. 
                    We then use (c) <b>Multi-Scale Decoder</b> to aggregate the resulting query clip’s mask features Y<sup>q</sup>,enc with clip’s frame features X<sup>q</sup> using Pixel Decoder to give contexualized feature pyramid Y<sup>q,fpn</sup>. 
                    Finally, we use <b>Space-Time Decoder</b> to decode mask predictions Y<sup>q</sup> , by refining learned time embeddings on the contextualized feature pyramid Y<sup>q,fpn</sup>. 
                    We update the memory with the predictions from last frame (=L<sup>th</sup> index) in the query clip, implemented as FIFO queue. 
                    During training, we use our transformation-aware loss L<sup>tr</sup> to form segmentation loss for the entire video.
                </p>
            </div>
        </div>

        <!-- Results -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on VOST
                </h3>
                <p class="text-justify">
                    The videos qualitatively compare M<sup>3</sup>T with <a href="https://github.com/z-x-yang/AOT" target="_blank" rel="noopener noreferrer">AOT</a>.
                </p>        
            </div>
        </div>
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <table width="100%">
                    <tbody>
                        <tr>
                            <td align="center" valign="top" width="4%">
                                <h4> </h4>
                            </td>
                            <td align="center" valign="top" width="33%">
                                <h4> GT </h4>
                            </td>
                            <td align="center" valign="top" width="33%">
                                <h4> AOT </h4>
                            </td>
                            <td align="center" valign="top" width="33%">
                                <h4> Ours </h4>
                            </td>
                        </tr>
                        <tr>
                            <td align="center" valign="center" width="4%">
                                <h4> Ex. 1 </h4>
                            </td>
                            <td align="center" valign="top" width="33%">
                                <iframe src="https://www.youtube.com/embed/OL7TanGNdMU?controls=0&showinfo=0&autoplay=1&loop=1&mute=1&playlist=OL7TanGNdMU" allowfullscreen style="width:100%;"></iframe>
                            </td>
                            <td align="center" valign="top" width="33%">
                                <iframe src="https://www.youtube.com/embed/wD0Xb4rb1Ok?controls=0&showinfo=0&autoplay=1&loop=1&mute=1&playlist=wD0Xb4rb1Ok" allowfullscreen style="width:100%;"></iframe>
                            </td>
                            <td align="center" valign="top" width="33%">
                                <iframe src="https://www.youtube.com/embed/DDvZay6kxkE?controls=0&showinfo=0&autoplay=1&loop=1&mute=1&playlist=DDvZay6kxkE" allowfullscreen style="width:100%;"></iframe>
                            </td>
                        </tr>
                    </tbody>
                </table>       
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on VISOR
                </h3>
                <p class="text-justify">
                    The videos qualitatively compare NeurMips with <a href="https://github.com/facebookresearch/pytorch3d/tree/main/projects/nerf" target="_blank" rel="noopener noreferrer">NeRF</a> (PyTorch3D implementation) and <a href="https://nex-mpi.github.io/" target="_blank" rel="noopener noreferrer">NeX</a>, which are volume rendering method and multi-plane image (MPI) respectively.
                </p>
                <br>
                <div class="row">
                    <div class="col-md-4 text-center">
                        <ul class="nav nav-pills nav-justified">
                            <li>
                                <a href="https://youtu.be/RO6g-9pA0rs" target="_blank" rel="noopener noreferrer">
                                <image src="img/replica/replica-apartment0.jpg" height="150px">
                                    <h4><strong>Apartment 0</strong></h4>
                                </a>
                            </li>
                            <li>
                                <a href="https://youtu.be/QGGRkbwnM44" target="_blank" rel="noopener noreferrer">
                                <image src="img/replica/replica-apartment1.jpg" height="150px">
                                    <h4><strong>Apartment 1</strong></h4>
                                </a>
                            </li>
                            <li>
                                <a href="https://youtu.be/eGlpLuT-1f8" target="_blank" rel="noopener noreferrer">
                                <image src="img/replica/replica-apartment2.jpg" height="150px">
                                    <h4><strong>Apartment 2</strong></h4>
                                </a>
                            </li>
                            <li>
                                <a href="https://youtu.be/6jIc0f0eOfk" target="_blank" rel="noopener noreferrer">
                                <image src="img/replica/replica-frl0.jpg" height="150px">
                                    <h4><strong>FRL 0</strong></h4>
                                </a>
                            </li>
                            <br>
                            <li>
                                <a href="https://youtu.be/GqMEzrauJkE" target="_blank" rel="noopener noreferrer">
                                <image src="img/replica/replica-kitchen.jpg" height="150px">
                                    <h4><strong>Kitchen</strong></h4>
                                </a>
                            </li>
                            <li>
                                <a href="https://youtu.be/zZ-1nrIK_l8" target="_blank" rel="noopener noreferrer">
                                <image src="img/replica/replica-room0.jpg" height="150px">
                                    <h4><strong>Room 0</strong></h4>
                                </a>
                            </li>
                            <li>
                                <a href="https://youtu.be/4k_3FGSFdMA" target="_blank" rel="noopener noreferrer">
                                <image src="img/replica/replica-room2.jpg" height="150px">
                                    <h4><strong>Room 2</strong></h4>
                                </a>
                            </li>
                        </ul>
                    </div>
                </div>
  
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    The M<sup>3</sup>T is implemented on top of the codebase of <a href="https://github.com/antoyang/TubeDETR" target="_blank" rel="noopener noreferrer">TubeDETR</a>, 
                    <a href="https://github.com/facebookresearch/Mask2Former" target="_blank" rel="noopener noreferrer">Mask2Former</a>, 
                    and <a href="https://github.com/TRI-ML/VOST" target="_blank" rel="noopener noreferrer">AOT in VOST</a>.
                </p>
               
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-10 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-0">
                    <textarea id="bibtex" class="form-control" readonly>
@article{goyal2023m3t,
    title={M3T: Multi-Scale Memory Matching for Video Object Segmentation and Tracking},
    author={Goyal, Raghav and Fan, Wan-Cyuan and Siam, Mennatullah and Sigal, Leonid},
    journal={arXiv preprint arXiv:2312.08514},
    year={2023}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    This work was funded, in part, by the Vector Institute for AI,
                    Canada CIFAR AI Chairs, NSERC CRC, and NSERC DGs. Resources used in preparing this research were provided, in part, by
                    the Province of Ontario, the Government of Canada through CIFAR, the Digital Research Alliance of Canada
                    , companies sponsoring the Vector Institute, and Advanced Research Computing at
                    the University of British Columbia. Additional hardware support
                    was provided by John R. Evans Leaders Fund CFI grant and Compute Canada under the Resource Allocation Competition award.
                    Additionally, we would like to thank <a href="https://pvtokmakov.github.io/home/" target="_blank">Pavel Tokmakov</a>, the author of the VOST datasets, 
                    for his invaluable assistance with the experimental results related to VOST.
                    The website template was borrowed from <a href="http://mgharbi.com/" target="_blank">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
